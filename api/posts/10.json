{"total":112,"pageSize":10,"pageCount":12,"data":[{"title":"昆士兰科技大学_20201225_SWA Object Detection","slug":"Paper017","date":"2021-01-13T14:40:56.000Z","updated":"2021-03-20T06:33:55.000Z","comments":true,"path":"api/articles/Paper017.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_017-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"随机加权平均","path":"api/tags/随机加权平均.json"},{"name":"周期性学习率","path":"api/tags/周期性学习率.json"}]},{"title":"百度_20141219_Deep Speech：Scaling up end-to-end speech recognition","slug":"Paper016","date":"2021-01-11T14:45:26.000Z","updated":"2021-01-11T14:53:54.000Z","comments":true,"path":"api/articles/Paper016.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_016-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"N-Gram","path":"api/tags/N-Gram.json"}]},{"title":"MERL_2017_Hybrid CTC/Attention Architecture for End-to-End Speech Recognition","slug":"Paper015","date":"2021-01-10T15:17:45.000Z","updated":"2021-01-11T15:10:42.000Z","comments":true,"path":"api/articles/Paper015.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_015-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"快速使用阿里云语音识别和合成API服务","slug":"Blog004","date":"2021-01-09T12:18:00.000Z","updated":"2021-03-03T04:06:02.000Z","comments":true,"path":"api/articles/Blog004.json","excerpt":null,"keywords":null,"cover":"/img/blogs/blog_004-01.jpg","content":null,"raw":null,"categories":[{"name":"Blog","path":"api/categories/Blog.json"}],"tags":[{"name":"Python","path":"api/tags/Python.json"},{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"阿里云服务","path":"api/tags/阿里云服务.json"}]},{"title":"MERL_ICASSP2017_Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning","slug":"Paper014","date":"2021-01-09T09:45:40.000Z","updated":"2021-03-23T09:43:54.000Z","comments":true,"path":"api/articles/Paper014.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_014-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"约翰斯·霍普金斯大学_Interspeech2018_ESPnet：End-to-End Speech Processing Toolkit","slug":"Paper013","date":"2021-01-07T01:28:32.000Z","updated":"2021-01-07T02:09:33.000Z","comments":true,"path":"api/articles/Paper013.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_013-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"HMM","path":"api/tags/HMM.json"},{"name":"Kaldi","path":"api/tags/Kaldi.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"DNN","path":"api/tags/DNN.json"},{"name":"TDNN","path":"api/tags/TDNN.json"},{"name":"BLSTM","path":"api/tags/BLSTM.json"},{"name":"LF-MMI","path":"api/tags/LF-MMI.json"},{"name":"Chain","path":"api/tags/Chain.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNNLM","path":"api/tags/RNNLM.json"}]},{"title":"东京大学_AAAI2021_Towards Fully Automated Manga Translation","slug":"Paper012","date":"2021-01-05T15:10:59.000Z","updated":"2021-01-05T15:23:32.000Z","comments":true,"path":"api/articles/Paper012.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_012-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"机器翻译","path":"api/tags/机器翻译.json"},{"name":"漫画翻译","path":"api/tags/漫画翻译.json"},{"name":"文字检测","path":"api/tags/文字检测.json"},{"name":"文字识别","path":"api/tags/文字识别.json"},{"name":"文字嵌入","path":"api/tags/文字嵌入.json"}]},{"title":"查尔斯大学_Interspeech2020_One Model, Many Languages：Meta-learning for Multilingual Text-to-Speech","slug":"Paper011","date":"2020-12-28T15:12:23.000Z","updated":"2021-05-27T03:30:02.000Z","comments":true,"path":"api/articles/Paper011.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_011-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Tacotron2","path":"api/tags/Tacotron2.json"},{"name":"多语言/跨语言","path":"api/tags/多语言/跨语言.json"},{"name":"WaveRNN","path":"api/tags/WaveRNN.json"}]},{"title":"台湾大学_Interspeech2019_One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization","slug":"Paper010","date":"2020-12-28T14:56:11.000Z","updated":"2021-05-27T03:25:24.000Z","comments":true,"path":"api/articles/Paper010.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_010-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"},{"name":"声纹编码器","path":"api/tags/声纹编码器.json"},{"name":"内容编码器","path":"api/tags/内容编码器.json"},{"name":"Instance Normalization","path":"api/tags/Instance Normalization.json"}]},{"title":"香港中文大学_2016_Phonetic posteriorgrams for many-to-one voice conversion without parallel data training","slug":"Paper009","date":"2020-12-28T14:29:20.000Z","updated":"2021-05-27T04:11:39.000Z","comments":true,"path":"api/articles/Paper009.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_009-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"语音后验图","path":"api/tags/语音后验图.json"},{"name":"MCEP","path":"api/tags/MCEP.json"},{"name":"DTW","path":"api/tags/DTW.json"},{"name":"DBLSTM","path":"api/tags/DBLSTM.json"},{"name":"Straight","path":"api/tags/Straight.json"}]}]}