{"name":"Paper","postlist":[{"title":"谷歌_Interspeech2017_Tacotron：Towards End-to-End Speech Synthesis","slug":"Paper001","date":"2020-12-23T12:46:14.000Z","updated":"2021-03-29T07:35:19.000Z","comments":true,"path":"api/articles/Paper001.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_001-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"}]},{"title":"谷歌_ICASSP2018_Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions","slug":"Paper002","date":"2020-12-26T03:02:03.000Z","updated":"2021-03-29T09:53:28.000Z","comments":true,"path":"api/articles/Paper002.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_002-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Tacotron2","path":"api/tags/Tacotron2.json"},{"name":"WaveNet","path":"api/tags/WaveNet.json"}]},{"title":"浙江大学_NeurIP2019_FastSpeech：Fast, Robust and Controllable Text to Speech","slug":"Paper003","date":"2020-12-26T03:45:22.000Z","updated":"2021-03-29T10:05:26.000Z","comments":true,"path":"api/articles/Paper003.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_003-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"FastSpeech","path":"api/tags/FastSpeech.json"},{"name":"WaveGlow","path":"api/tags/WaveGlow.json"}]},{"title":"浙江大学_20200622_FastSpeech 2：Fast and High-Quality End-to-End Text to Speech","slug":"Paper004","date":"2020-12-26T03:45:27.000Z","updated":"2021-03-29T10:08:55.000Z","comments":true,"path":"api/articles/Paper004.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_004-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"WaveGlow","path":"api/tags/WaveGlow.json"},{"name":"FastSpeech2","path":"api/tags/FastSpeech2.json"}]},{"title":"谷歌_NeurIPS2018_Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis","slug":"Paper005","date":"2020-12-26T03:45:48.000Z","updated":"2020-12-28T13:52:38.000Z","comments":true,"path":"api/articles/Paper005.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_005-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"说话人风格迁移","path":"api/tags/说话人风格迁移.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Tacotron2","path":"api/tags/Tacotron2.json"},{"name":"WaveNet","path":"api/tags/WaveNet.json"},{"name":"声纹编码器","path":"api/tags/声纹编码器.json"}]},{"title":"斯坦福大学_2018_Storytime - End to end neural networks for audiobooks","slug":"Paper006","date":"2020-12-27T13:25:08.000Z","updated":"2021-01-07T02:21:59.000Z","comments":true,"path":"api/articles/Paper006.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_006-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"}]},{"title":"谷歌_ICML2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron","slug":"Paper007","date":"2020-12-28T13:56:52.000Z","updated":"2020-12-28T14:12:40.000Z","comments":true,"path":"api/articles/Paper007.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_007-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"说话人风格迁移","path":"api/tags/说话人风格迁移.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"},{"name":"WaveNet","path":"api/tags/WaveNet.json"},{"name":"韵律编码器","path":"api/tags/韵律编码器.json"}]},{"title":"谷歌_20180323_Style Tokens：Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis","slug":"Paper008","date":"2020-12-28T14:12:56.000Z","updated":"2020-12-28T14:25:54.000Z","comments":true,"path":"api/articles/Paper008.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_008-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"说话人风格迁移","path":"api/tags/说话人风格迁移.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"},{"name":"WaveNet","path":"api/tags/WaveNet.json"},{"name":"声纹编码器","path":"api/tags/声纹编码器.json"},{"name":"韵律编码器","path":"api/tags/韵律编码器.json"}]},{"title":"香港中文大学_2016_Phonetic posteriorgrams for many-to-one voice conversion without parallel data training","slug":"Paper009","date":"2020-12-28T14:29:20.000Z","updated":"2021-05-27T04:11:39.000Z","comments":true,"path":"api/articles/Paper009.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_009-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"语音后验图","path":"api/tags/语音后验图.json"},{"name":"MCEP","path":"api/tags/MCEP.json"},{"name":"DTW","path":"api/tags/DTW.json"},{"name":"DBLSTM","path":"api/tags/DBLSTM.json"},{"name":"Straight","path":"api/tags/Straight.json"}]},{"title":"台湾大学_Interspeech2019_One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization","slug":"Paper010","date":"2020-12-28T14:56:11.000Z","updated":"2021-05-27T03:25:24.000Z","comments":true,"path":"api/articles/Paper010.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_010-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"},{"name":"声纹编码器","path":"api/tags/声纹编码器.json"},{"name":"内容编码器","path":"api/tags/内容编码器.json"},{"name":"Instance Normalization","path":"api/tags/Instance Normalization.json"}]},{"title":"查尔斯大学_Interspeech2020_One Model, Many Languages：Meta-learning for Multilingual Text-to-Speech","slug":"Paper011","date":"2020-12-28T15:12:23.000Z","updated":"2021-05-27T03:30:02.000Z","comments":true,"path":"api/articles/Paper011.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_011-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Tacotron2","path":"api/tags/Tacotron2.json"},{"name":"多语言/跨语言","path":"api/tags/多语言/跨语言.json"},{"name":"WaveRNN","path":"api/tags/WaveRNN.json"}]},{"title":"东京大学_AAAI2021_Towards Fully Automated Manga Translation","slug":"Paper012","date":"2021-01-05T15:10:59.000Z","updated":"2021-01-05T15:23:32.000Z","comments":true,"path":"api/articles/Paper012.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_012-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"机器翻译","path":"api/tags/机器翻译.json"},{"name":"漫画翻译","path":"api/tags/漫画翻译.json"},{"name":"文字检测","path":"api/tags/文字检测.json"},{"name":"文字识别","path":"api/tags/文字识别.json"},{"name":"文字嵌入","path":"api/tags/文字嵌入.json"}]},{"title":"约翰斯·霍普金斯大学_Interspeech2018_ESPnet：End-to-End Speech Processing Toolkit","slug":"Paper013","date":"2021-01-07T01:28:32.000Z","updated":"2021-01-07T02:09:33.000Z","comments":true,"path":"api/articles/Paper013.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_013-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"HMM","path":"api/tags/HMM.json"},{"name":"Kaldi","path":"api/tags/Kaldi.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"DNN","path":"api/tags/DNN.json"},{"name":"TDNN","path":"api/tags/TDNN.json"},{"name":"BLSTM","path":"api/tags/BLSTM.json"},{"name":"LF-MMI","path":"api/tags/LF-MMI.json"},{"name":"Chain","path":"api/tags/Chain.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNNLM","path":"api/tags/RNNLM.json"}]},{"title":"MERL_ICASSP2017_Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning","slug":"Paper014","date":"2021-01-09T09:45:40.000Z","updated":"2021-03-23T09:43:54.000Z","comments":true,"path":"api/articles/Paper014.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_014-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"MERL_2017_Hybrid CTC/Attention Architecture for End-to-End Speech Recognition","slug":"Paper015","date":"2021-01-10T15:17:45.000Z","updated":"2021-01-11T15:10:42.000Z","comments":true,"path":"api/articles/Paper015.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_015-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"百度_20141219_Deep Speech：Scaling up end-to-end speech recognition","slug":"Paper016","date":"2021-01-11T14:45:26.000Z","updated":"2021-01-11T14:53:54.000Z","comments":true,"path":"api/articles/Paper016.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_016-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"N-Gram","path":"api/tags/N-Gram.json"}]},{"title":"昆士兰科技大学_20201225_SWA Object Detection","slug":"Paper017","date":"2021-01-13T14:40:56.000Z","updated":"2021-03-20T06:33:55.000Z","comments":true,"path":"api/articles/Paper017.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_017-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"随机加权平均","path":"api/tags/随机加权平均.json"},{"name":"周期性学习率","path":"api/tags/周期性学习率.json"}]},{"title":"康奈尔大学_UAI2018_Averaging Weights Leads to Wider Optima and Better Generalization","slug":"Paper018","date":"2021-01-15T17:04:58.000Z","updated":"2021-03-20T06:34:08.000Z","comments":true,"path":"api/articles/Paper018.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_018-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"随机加权平均","path":"api/tags/随机加权平均.json"},{"name":"周期性学习率","path":"api/tags/周期性学习率.json"},{"name":"随机梯度下降","path":"api/tags/随机梯度下降.json"}]},{"title":"百度_ICML2015_Deep Speech 2：End-to-End Speech Recognition in English and Mandarin","slug":"Paper019","date":"2021-01-18T14:39:52.000Z","updated":"2021-03-20T08:14:55.000Z","comments":true,"path":"api/articles/Paper019.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_019-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"GRU","path":"api/tags/GRU.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"KenLM","path":"api/tags/KenLM.json"}]},{"title":"百度_ASRU2017_Exploring Neural Transducers for End-to-End Speech Recognition","slug":"Paper020","date":"2021-01-19T14:30:36.000Z","updated":"2021-01-19T15:03:41.000Z","comments":true,"path":"api/articles/Paper020.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_020-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN-T","path":"api/tags/RNN-T.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"}]},{"title":"百度_20171105_Robust Speech Recognition Using Generative Adversarial Networks","slug":"Paper021","date":"2021-01-19T14:37:30.000Z","updated":"2021-01-19T14:46:39.000Z","comments":true,"path":"api/articles/Paper021.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_021-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语音增强","path":"api/tags/语音增强.json"},{"name":"SEGAN","path":"api/tags/SEGAN.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"WGAN","path":"api/tags/WGAN.json"}]},{"title":"百度_Interspeech2018_Cold Fusion：Training Seq2Seq Models Together with Language Models","slug":"Paper022","date":"2021-01-19T14:37:35.000Z","updated":"2021-01-19T15:03:41.000Z","comments":true,"path":"api/articles/Paper022.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_022-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"Deep Fusion","path":"api/tags/Deep Fusion.json"},{"name":"Cold Fusion","path":"api/tags/Cold Fusion.json"}]},{"title":"出门问问_20201210_Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition","slug":"Paper023","date":"2021-01-23T10:26:31.000Z","updated":"2021-03-31T06:19:54.000Z","comments":true,"path":"api/articles/Paper023.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_023-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"}]},{"title":"出门问问_ICASSP2019_End-To-End Speech Recognition Using A High Rank LSTM-CTC Based Model","slug":"Paper024","date":"2021-01-24T02:46:46.000Z","updated":"2021-03-31T06:18:50.000Z","comments":true,"path":"api/articles/Paper024.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_024-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"LSTM","path":"api/tags/LSTM.json"}]},{"title":"出门问问_ICASSP2019_Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization","slug":"Paper025","date":"2021-01-24T03:24:38.000Z","updated":"2021-01-24T03:35:30.000Z","comments":true,"path":"api/articles/Paper025.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_025-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"RNNLM","path":"api/tags/RNNLM.json"}]},{"title":"出门问问_ICASSP2019_Adversarial Examples for Improving End-to-end Attention-based Small-Footprint Keyword Spotting","slug":"Paper026","date":"2021-01-24T03:25:40.000Z","updated":"2021-01-24T03:41:13.000Z","comments":true,"path":"api/articles/Paper026.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_026-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音唤醒","path":"api/tags/语音唤醒.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"}]},{"title":"谷歌_ICASSP2018_Generalized End-to-End Loss for Speaker Verification","slug":"Paper027","date":"2021-01-27T15:02:39.000Z","updated":"2021-01-27T15:11:39.000Z","comments":true,"path":"api/articles/Paper027.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_027-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"GE2E","path":"api/tags/GE2E.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"谷歌_Interspeech2020_Conformer：Convolution-augmented Transformer for Speech Recognition","slug":"Paper028","date":"2021-02-02T02:04:40.000Z","updated":"2021-02-02T02:14:18.000Z","comments":true,"path":"api/articles/Paper028.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_028-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"Transformer","path":"api/tags/Transformer.json"},{"name":"Conformer","path":"api/tags/Conformer.json"}]},{"title":"DFKI_ICASSP2016_End-to-End Text-Dependent Speaker Verification","slug":"Paper029","date":"2021-02-14T04:38:27.000Z","updated":"2021-02-14T05:22:07.000Z","comments":true,"path":"api/articles/Paper029.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_029-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"华盛顿州立大学_ICASSP2018_Attention-Based Models for Text-Dependent Speaker Verification","slug":"Paper030","date":"2021-02-14T09:35:26.000Z","updated":"2021-02-14T09:49:10.000Z","comments":true,"path":"api/articles/Paper030.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_030-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"CMU_ICASSP2016_Listen, Attend and Spell","slug":"Paper031","date":"2021-02-19T08:44:24.000Z","updated":"2021-02-19T09:31:22.000Z","comments":true,"path":"api/articles/Paper031.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_031-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"BLSTM","path":"api/tags/BLSTM.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"LAS","path":"api/tags/LAS.json"}]},{"title":"多伦多大学_ICML2006_Connectionist temporal classification：labelling unsegmented sequence data with recurrent neural networks","slug":"Paper032","date":"2021-02-19T14:33:33.000Z","updated":"2021-02-19T15:16:42.000Z","comments":true,"path":"api/articles/Paper032.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_032-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"多伦多大学_ICML2012_Sequence Transduction with Recurrent Neural Networks","slug":"Paper033","date":"2021-02-21T08:50:48.000Z","updated":"2021-02-21T09:07:23.000Z","comments":true,"path":"api/articles/Paper033.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_033-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"RNN-T","path":"api/tags/RNN-T.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"}]},{"title":"谷歌_Interspeech2017_Recurrent Neural Aligner：An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping","slug":"Paper034","date":"2021-02-21T09:20:55.000Z","updated":"2021-02-21T09:24:46.000Z","comments":true,"path":"api/articles/Paper034.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_034-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"RNA","path":"api/tags/RNA.json"}]},{"title":"谷歌_20160804_A Neural Transducer","slug":"Paper035","date":"2021-02-21T09:28:16.000Z","updated":"2021-02-21T09:44:48.000Z","comments":true,"path":"api/articles/Paper035.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_035-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"RNN-T","path":"api/tags/RNN-T.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"}]},{"title":"谷歌_ICLR2018_Monotonic Chunkwise Attention","slug":"Paper036","date":"2021-02-21T09:28:16.000Z","updated":"2021-02-21T09:48:16.000Z","comments":true,"path":"api/articles/Paper036.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_036-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"MoChA","path":"api/tags/MoChA.json"}]},{"title":"加泰罗尼亚理工大学_Interspeech2017_SEGAN：Speech Enhancement Generative Adversarial Network","slug":"Paper037","date":"2021-03-02T15:30:33.000Z","updated":"2021-06-15T13:41:03.000Z","comments":true,"path":"api/articles/Paper037.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_037-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音增强","path":"api/tags/语音增强.json"},{"name":"GAN","path":"api/tags/GAN.json"},{"name":"SEGAN","path":"api/tags/SEGAN.json"}]},{"title":"谷歌_Interspeech2019_SpecAugment：A Simple Data Augmentation Method for Automatic Speech Recognition","slug":"Paper038","date":"2021-03-04T03:14:29.000Z","updated":"2021-03-20T06:39:32.000Z","comments":true,"path":"api/articles/Paper038.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_038-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音增强","path":"api/tags/语音增强.json"},{"name":"LAS","path":"api/tags/LAS.json"},{"name":"SpecAugment","path":"api/tags/SpecAugment.json"}]},{"title":"出门问问_20210202_WeNet：Production First and Production Ready End-to-End Speech Recognition Toolkit","slug":"Paper039","date":"2021-03-31T14:46:22.000Z","updated":"2021-03-31T14:57:54.000Z","comments":true,"path":"api/articles/Paper039.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_039-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"端到端","path":"api/tags/端到端.json"}]},{"title":"华盛顿大学_20160509_You Only Look Once：Unified, Real-Time Object Detection","slug":"Paper040","date":"2021-04-30T05:34:39.000Z","updated":"2021-04-30T06:00:31.000Z","comments":true,"path":"api/articles/Paper040.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_040-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"YOLO","path":"api/tags/YOLO.json"}]},{"title":"康奈尔大学_CVPR2017_Densely Connected Convolutional Networks","slug":"Paper041","date":"2021-05-07T06:59:59.000Z","updated":"2021-05-07T07:38:47.000Z","comments":true,"path":"api/articles/Paper041.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_041-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"DenseNet","path":"api/tags/DenseNet.json"}]},{"title":"微软_CVPR2016_Deep Residual Learning for Image Recognition","slug":"Paper042","date":"2021-05-08T04:44:54.000Z","updated":"2021-05-08T05:48:15.000Z","comments":true,"path":"api/articles/Paper042.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_042-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"ResNet","path":"api/tags/ResNet.json"}]},{"title":"谷歌_CVPR2015_Going Deeper with Convolutions","slug":"Paper043","date":"2021-05-10T07:06:20.000Z","updated":"2021-05-10T07:24:12.000Z","comments":true,"path":"api/articles/Paper043.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_043-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"Inception","path":"api/tags/Inception.json"},{"name":"GoogLeNet","path":"api/tags/GoogLeNet.json"}]},{"title":"谷歌_20150302_Batch Normalization：Accelerating Deep Network Training by Reducing Internal Covariate Shift","slug":"Paper044","date":"2021-05-10T07:08:53.000Z","updated":"2021-05-10T07:32:23.000Z","comments":true,"path":"api/articles/Paper044.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_044-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"Inception","path":"api/tags/Inception.json"},{"name":"BatchNormalization","path":"api/tags/BatchNormalization.json"}]},{"title":"谷歌_CVPR2016_Rethinking the Inception Architecture for Computer Vision","slug":"Paper045","date":"2021-05-10T07:09:42.000Z","updated":"2021-05-10T07:36:12.000Z","comments":true,"path":"api/articles/Paper045.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_045-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"Inception","path":"api/tags/Inception.json"}]},{"title":"谷歌_20160223_Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning","slug":"Paper046","date":"2021-05-10T07:10:24.000Z","updated":"2021-05-10T07:40:19.000Z","comments":true,"path":"api/articles/Paper046.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_046-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"Inception","path":"api/tags/Inception.json"}]},{"title":"百度_20181012_Neural Voice Cloning with a Few Samples","slug":"Paper047","date":"2021-05-31T06:19:03.000Z","updated":"2021-05-31T06:28:17.000Z","comments":true,"path":"api/articles/Paper047.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_047-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"}]},{"title":"西班牙电话公司_NeurIPS2019_Blow：a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion","slug":"Paper048","date":"2021-05-31T06:19:38.000Z","updated":"2021-06-01T02:20:29.000Z","comments":true,"path":"api/articles/Paper048.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_048-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"}]},{"title":"清华大学_20191120_CAT：CRF-based ASR Toolkit","slug":"Paper049","date":"2021-06-02T02:29:03.000Z","updated":"2021-06-15T13:58:28.000Z","comments":true,"path":"api/articles/Paper049.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_049-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"}]},{"title":"华中科技大学_AAAI2020_Real-time Scene Text Detection with Differentiable Binarization","slug":"Paper050","date":"2021-06-15T12:49:02.000Z","updated":"2021-06-15T13:39:20.000Z","comments":true,"path":"api/articles/Paper050.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_050-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"文本检测","path":"api/tags/文本检测.json"}]}]}