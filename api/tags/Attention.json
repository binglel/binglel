{"name":"Attention","postlist":[{"title":"约翰斯·霍普金斯大学_Interspeech2018_ESPnet：End-to-End Speech Processing Toolkit","slug":"Paper013","date":"2021-01-07T01:28:32.000Z","updated":"2021-01-07T02:09:33.000Z","comments":true,"path":"api/articles/Paper013.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_013-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"HMM","path":"api/tags/HMM.json"},{"name":"Kaldi","path":"api/tags/Kaldi.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"DNN","path":"api/tags/DNN.json"},{"name":"TDNN","path":"api/tags/TDNN.json"},{"name":"BLSTM","path":"api/tags/BLSTM.json"},{"name":"LF-MMI","path":"api/tags/LF-MMI.json"},{"name":"Chain","path":"api/tags/Chain.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNNLM","path":"api/tags/RNNLM.json"}]},{"title":"MERL_ICASSP2017_Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning","slug":"Paper014","date":"2021-01-09T09:45:40.000Z","updated":"2021-03-23T09:43:54.000Z","comments":true,"path":"api/articles/Paper014.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_014-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"MERL_2017_Hybrid CTC/Attention Architecture for End-to-End Speech Recognition","slug":"Paper015","date":"2021-01-10T15:17:45.000Z","updated":"2021-01-11T15:10:42.000Z","comments":true,"path":"api/articles/Paper015.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_015-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"百度_ASRU2017_Exploring Neural Transducers for End-to-End Speech Recognition","slug":"Paper020","date":"2021-01-19T14:30:36.000Z","updated":"2021-01-19T15:03:41.000Z","comments":true,"path":"api/articles/Paper020.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_020-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN-T","path":"api/tags/RNN-T.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"}]},{"title":"百度_20171105_Robust Speech Recognition Using Generative Adversarial Networks","slug":"Paper021","date":"2021-01-19T14:37:30.000Z","updated":"2021-01-19T14:46:39.000Z","comments":true,"path":"api/articles/Paper021.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_021-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语音增强","path":"api/tags/语音增强.json"},{"name":"SEGAN","path":"api/tags/SEGAN.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"WGAN","path":"api/tags/WGAN.json"}]},{"title":"百度_Interspeech2018_Cold Fusion：Training Seq2Seq Models Together with Language Models","slug":"Paper022","date":"2021-01-19T14:37:35.000Z","updated":"2021-01-19T15:03:41.000Z","comments":true,"path":"api/articles/Paper022.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_022-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"Deep Fusion","path":"api/tags/Deep Fusion.json"},{"name":"Cold Fusion","path":"api/tags/Cold Fusion.json"}]},{"title":"出门问问_20201210_Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition","slug":"Paper023","date":"2021-01-23T10:26:31.000Z","updated":"2021-03-31T06:19:54.000Z","comments":true,"path":"api/articles/Paper023.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_023-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"}]},{"title":"出门问问_ICASSP2019_Adversarial Examples for Improving End-to-end Attention-based Small-Footprint Keyword Spotting","slug":"Paper026","date":"2021-01-24T03:25:40.000Z","updated":"2021-01-24T03:41:13.000Z","comments":true,"path":"api/articles/Paper026.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_026-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音唤醒","path":"api/tags/语音唤醒.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"}]},{"title":"谷歌_Interspeech2020_Conformer：Convolution-augmented Transformer for Speech Recognition","slug":"Paper028","date":"2021-02-02T02:04:40.000Z","updated":"2021-02-02T02:14:18.000Z","comments":true,"path":"api/articles/Paper028.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_028-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"Transformer","path":"api/tags/Transformer.json"},{"name":"Conformer","path":"api/tags/Conformer.json"}]},{"title":"华盛顿州立大学_ICASSP2018_Attention-Based Models for Text-Dependent Speaker Verification","slug":"Paper030","date":"2021-02-14T09:35:26.000Z","updated":"2021-02-14T09:49:10.000Z","comments":true,"path":"api/articles/Paper030.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_030-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"CMU_ICASSP2016_Listen, Attend and Spell","slug":"Paper031","date":"2021-02-19T08:44:24.000Z","updated":"2021-02-19T09:31:22.000Z","comments":true,"path":"api/articles/Paper031.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_031-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"BLSTM","path":"api/tags/BLSTM.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"LAS","path":"api/tags/LAS.json"}]},{"title":"谷歌_ICLR2018_Monotonic Chunkwise Attention","slug":"Paper036","date":"2021-02-21T09:28:16.000Z","updated":"2021-02-21T09:48:16.000Z","comments":true,"path":"api/articles/Paper036.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_036-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"MoChA","path":"api/tags/MoChA.json"}]},{"title":"神经网络与深度学习_邱锡鹏_复旦大学","slug":"Tutorial005","date":"2021-02-26T04:24:56.000Z","updated":"2021-02-26T04:42:14.000Z","comments":true,"path":"api/articles/Tutorial005.json","excerpt":null,"keywords":null,"cover":"/img/tutorials/tutorial_005-01.jpg","content":null,"raw":null,"categories":[{"name":"Tutorial","path":"api/categories/Tutorial.json"}],"tags":[{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"深度学习","path":"api/tags/深度学习.json"}]}]}