{"name":"End-to-End","postlist":[{"title":"谷歌_Interspeech2017_Tacotron：Towards End-to-End Speech Synthesis","slug":"Paper001","date":"2020-12-23T12:46:14.000Z","updated":"2021-03-29T07:35:19.000Z","comments":true,"path":"api/articles/Paper001.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_001-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"}]},{"title":"谷歌_ICASSP2018_Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions","slug":"Paper002","date":"2020-12-26T03:02:03.000Z","updated":"2021-03-29T09:53:28.000Z","comments":true,"path":"api/articles/Paper002.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_002-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Tacotron2","path":"api/tags/Tacotron2.json"},{"name":"WaveNet","path":"api/tags/WaveNet.json"}]},{"title":"斯坦福大学_2018_Storytime - End to end neural networks for audiobooks","slug":"Paper006","date":"2020-12-27T13:25:08.000Z","updated":"2021-01-07T02:21:59.000Z","comments":true,"path":"api/articles/Paper006.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_006-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"Tacotron","path":"api/tags/Tacotron.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"Griffin-Lim","path":"api/tags/Griffin-Lim.json"}]},{"title":"约翰斯·霍普金斯大学_Interspeech2018_ESPnet：End-to-End Speech Processing Toolkit","slug":"Paper013","date":"2021-01-07T01:28:32.000Z","updated":"2021-01-07T02:09:33.000Z","comments":true,"path":"api/articles/Paper013.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_013-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"HMM","path":"api/tags/HMM.json"},{"name":"Kaldi","path":"api/tags/Kaldi.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"DNN","path":"api/tags/DNN.json"},{"name":"TDNN","path":"api/tags/TDNN.json"},{"name":"BLSTM","path":"api/tags/BLSTM.json"},{"name":"LF-MMI","path":"api/tags/LF-MMI.json"},{"name":"Chain","path":"api/tags/Chain.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNNLM","path":"api/tags/RNNLM.json"}]},{"title":"MERL_ICASSP2017_Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning","slug":"Paper014","date":"2021-01-09T09:45:40.000Z","updated":"2021-03-23T09:43:54.000Z","comments":true,"path":"api/articles/Paper014.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_014-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"MERL_2017_Hybrid CTC/Attention Architecture for End-to-End Speech Recognition","slug":"Paper015","date":"2021-01-10T15:17:45.000Z","updated":"2021-01-11T15:10:42.000Z","comments":true,"path":"api/articles/Paper015.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_015-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN","path":"api/tags/RNN.json"}]},{"title":"百度_20141219_Deep Speech：Scaling up end-to-end speech recognition","slug":"Paper016","date":"2021-01-11T14:45:26.000Z","updated":"2021-01-11T14:53:54.000Z","comments":true,"path":"api/articles/Paper016.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_016-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"N-Gram","path":"api/tags/N-Gram.json"}]},{"title":"百度_ICML2015_Deep Speech 2：End-to-End Speech Recognition in English and Mandarin","slug":"Paper019","date":"2021-01-18T14:39:52.000Z","updated":"2021-03-20T08:14:55.000Z","comments":true,"path":"api/articles/Paper019.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_019-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"语言模型","path":"api/tags/语言模型.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"声学模型","path":"api/tags/声学模型.json"},{"name":"解码器","path":"api/tags/解码器.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"GRU","path":"api/tags/GRU.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"KenLM","path":"api/tags/KenLM.json"}]},{"title":"百度_ASRU2017_Exploring Neural Transducers for End-to-End Speech Recognition","slug":"Paper020","date":"2021-01-19T14:30:36.000Z","updated":"2021-01-19T15:03:41.000Z","comments":true,"path":"api/articles/Paper020.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_020-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"RNN-T","path":"api/tags/RNN-T.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"}]},{"title":"出门问问_20201210_Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition","slug":"Paper023","date":"2021-01-23T10:26:31.000Z","updated":"2021-03-31T06:19:54.000Z","comments":true,"path":"api/articles/Paper023.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_023-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"}]},{"title":"出门问问_ICASSP2019_End-To-End Speech Recognition Using A High Rank LSTM-CTC Based Model","slug":"Paper024","date":"2021-01-24T02:46:46.000Z","updated":"2021-03-31T06:18:50.000Z","comments":true,"path":"api/articles/Paper024.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_024-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"LSTM","path":"api/tags/LSTM.json"}]},{"title":"出门问问_ICASSP2019_Adversarial Examples for Improving End-to-end Attention-based Small-Footprint Keyword Spotting","slug":"Paper026","date":"2021-01-24T03:25:40.000Z","updated":"2021-01-24T03:41:13.000Z","comments":true,"path":"api/articles/Paper026.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_026-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音唤醒","path":"api/tags/语音唤醒.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"}]},{"title":"谷歌_ICASSP2018_Generalized End-to-End Loss for Speaker Verification","slug":"Paper027","date":"2021-01-27T15:02:39.000Z","updated":"2021-01-27T15:11:39.000Z","comments":true,"path":"api/articles/Paper027.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_027-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"GE2E","path":"api/tags/GE2E.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"谷歌_Interspeech2020_Conformer：Convolution-augmented Transformer for Speech Recognition","slug":"Paper028","date":"2021-02-02T02:04:40.000Z","updated":"2021-02-02T02:14:18.000Z","comments":true,"path":"api/articles/Paper028.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_028-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"Transformer","path":"api/tags/Transformer.json"},{"name":"Conformer","path":"api/tags/Conformer.json"}]},{"title":"DFKI_ICASSP2016_End-to-End Text-Dependent Speaker Verification","slug":"Paper029","date":"2021-02-14T04:38:27.000Z","updated":"2021-02-14T05:22:07.000Z","comments":true,"path":"api/articles/Paper029.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_029-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"华盛顿州立大学_ICASSP2018_Attention-Based Models for Text-Dependent Speaker Verification","slug":"Paper030","date":"2021-02-14T09:35:26.000Z","updated":"2021-02-14T09:49:10.000Z","comments":true,"path":"api/articles/Paper030.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_030-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"说话人认证","path":"api/tags/说话人认证.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"LSTM","path":"api/tags/LSTM.json"},{"name":"TE2E","path":"api/tags/TE2E.json"}]},{"title":"多伦多大学_ICML2006_Connectionist temporal classification：labelling unsegmented sequence data with recurrent neural networks","slug":"Paper032","date":"2021-02-19T14:33:33.000Z","updated":"2021-02-19T15:16:42.000Z","comments":true,"path":"api/articles/Paper032.json","excerpt":null,"keywords":null,"cover":"/img/papers/paper_032-01.jpg","content":null,"raw":null,"categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"RNN","path":"api/tags/RNN.json"}]}]}