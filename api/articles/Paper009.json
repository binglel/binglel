{"title":"香港中文大学_2016_Phonetic posteriorgrams for many-to-one voice conversion without parallel data training","slug":"Paper009","date":"2020-12-28T14:29:20.000Z","updated":"2021-05-27T04:11:39.000Z","comments":true,"path":"api/articles/Paper009.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>本文提出了一种基于非平行训练数据的新型语音转换方法。该想法是通过与说话者无关的自动语音识别（SI-ASR）系统获得的语音后验图（PPGs）来建立说话人之间关系。假设这些PPGs能够在说话人规范化空间中表示语音的清晰度，并与说话人独立地对应。所提出的方法首先获得目标语音的PPGs。使用基于循环神经网络的深度双向长短时间记忆（DBLSTM）结构来学习目标语音的声学特征之间和PPGs的关系。要转换任意的一段源语音，需要从相同的SI-ASR获得其PPGs并将它们输入到训练好的DBLSTM中以生成转换语音。我们的方法有两个主要优点：（1）需要非平行训练数据；（2）训练好的模型可以将任意一个源说话人的语音转换成目标说话人的语音（即多对一转换）。实验表明我们的方法性能与最先进的系统在语音质量和说话人相似度方面相同或优之。</p>\n<p><strong>参考</strong><br>[1] <em>Sun L, Li K, Wang H, et al. Phonetic posteriorgrams for many-to-one voice conversion without parallel data training[C]//2016 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2016: 1-6.</em><a href=\"http://www1.se.cuhk.edu.hk/~hccl/publications/pub/2016_paper_297.pdf\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>deep-voice-conversion</em><a href=\"https://github.com/andabi/deep-voice-conversion\">[GitHub]</a></p>\n","more":"<p><strong>摘要</strong><br>本文提出了一种基于非平行训练数据的新型语音转换方法。该想法是通过与说话者无关的自动语音识别（SI-ASR）系统获得的语音后验图（PPGs）来建立说话人之间关系。假设这些PPGs能够在说话人规范化空间中表示语音的清晰度，并与说话人独立地对应。所提出的方法首先获得目标语音的PPGs。使用基于循环神经网络的深度双向长短时间记忆（DBLSTM）结构来学习目标语音的声学特征之间和PPGs的关系。要转换任意的一段源语音，需要从相同的SI-ASR获得其PPGs并将它们输入到训练好的DBLSTM中以生成转换语音。我们的方法有两个主要优点：（1）需要非平行训练数据；（2）训练好的模型可以将任意一个源说话人的语音转换成目标说话人的语音（即多对一转换）。实验表明我们的方法性能与最先进的系统在语音质量和说话人相似度方面相同或优之。</p>\n<p><strong>参考</strong><br>[1] <em>Sun L, Li K, Wang H, et al. Phonetic posteriorgrams for many-to-one voice conversion without parallel data training[C]//2016 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2016: 1-6.</em><a href=\"http://www1.se.cuhk.edu.hk/~hccl/publications/pub/2016_paper_297.pdf\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>deep-voice-conversion</em><a href=\"https://github.com/andabi/deep-voice-conversion\">[GitHub]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"语音转换","path":"api/tags/语音转换.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"语音后验图","path":"api/tags/语音后验图.json"},{"name":"MCEP","path":"api/tags/MCEP.json"},{"name":"DTW","path":"api/tags/DTW.json"},{"name":"DBLSTM","path":"api/tags/DBLSTM.json"},{"name":"Straight","path":"api/tags/Straight.json"}]}