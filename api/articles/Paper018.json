{"title":"康奈尔大学_UAI2018_Averaging Weights Leads to Wider Optima and Better Generalization","slug":"Paper018","date":"2021-01-15T17:04:58.000Z","updated":"2021-03-20T06:34:08.000Z","comments":true,"path":"api/articles/Paper018.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>深度神经网络通常通过优化具有SGD变量的损失函数，结合衰减的学习率来训练，直到收敛。我们发现，在使用周期性或固定学习率情况下，沿着SGD轨迹的多个点的简单平均比传统的训练具有更好的泛化能力。我们还发现，这种随机加权平均(SWA)方法得到了比SGD方法更平坦的解，并且用一个单一模型逼近了最近的快速几何集成(FGE)方法。我们把SWA用在CIFAR-10、CIFAR-100和ImageNet中一系列最先进的残差网络、金字塔网、DenseNets和Shake-Shake网络上，其测试精度比传统的SGD训练获得了显著改进。简而言之，SWA非常容易实现，提高了通用性，并且几乎没有计算开销。</p>\n<p><strong>参考</strong><br>[1] <em>Izmailov P, Podoprikhin D, Garipov T, et al. Averaging weights leads to wider optima and better generalization[J]. arXiv preprint arXiv:1803.05407, 2018.</em><a href=\"https://arxiv.org/abs/1803.05407\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>swa</em><a href=\"https://github.com/timgaripov/swa\">[GitHub]</a></p>\n","more":"<p><strong>摘要</strong><br>深度神经网络通常通过优化具有SGD变量的损失函数，结合衰减的学习率来训练，直到收敛。我们发现，在使用周期性或固定学习率情况下，沿着SGD轨迹的多个点的简单平均比传统的训练具有更好的泛化能力。我们还发现，这种随机加权平均(SWA)方法得到了比SGD方法更平坦的解，并且用一个单一模型逼近了最近的快速几何集成(FGE)方法。我们把SWA用在CIFAR-10、CIFAR-100和ImageNet中一系列最先进的残差网络、金字塔网、DenseNets和Shake-Shake网络上，其测试精度比传统的SGD训练获得了显著改进。简而言之，SWA非常容易实现，提高了通用性，并且几乎没有计算开销。</p>\n<p><strong>参考</strong><br>[1] <em>Izmailov P, Podoprikhin D, Garipov T, et al. Averaging weights leads to wider optima and better generalization[J]. arXiv preprint arXiv:1803.05407, 2018.</em><a href=\"https://arxiv.org/abs/1803.05407\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>swa</em><a href=\"https://github.com/timgaripov/swa\">[GitHub]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"随机加权平均","path":"api/tags/随机加权平均.json"},{"name":"周期性学习率","path":"api/tags/周期性学习率.json"},{"name":"随机梯度下降","path":"api/tags/随机梯度下降.json"}]}