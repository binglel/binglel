{"title":"浙江大学_NeurIP2019_FastSpeech：Fast, Robust and Controllable Text to Speech","slug":"Paper003","date":"2020-12-26T03:45:22.000Z","updated":"2021-03-29T10:05:26.000Z","comments":true,"path":"api/articles/Paper003.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>基于神经网络的端到端文本到语音（TTS）大大提高了合成语音的质量。突出的方法（例如Tacotron2）通常首先从文本生成梅尔频谱图，然后使用诸如WaveNet的声码器从梅尔频谱图合成语音。与传统的拼接和统计参数方法相比，基于神经网络的端到端模型的推理速度较慢，并且合成语音通常不稳定（即某些单词被跳过或重复）且缺乏可控性（语音速度或韵律控制）。在这项工作中，我们提出了一种基于Transformer的新型前馈网络，可为TTS并行生成梅尔频谱图。具体来说，我们从基于编码器-解码器的教师模型中提取注意力对齐，以进行音素持续时间预测，长度调节器将其用于扩展源音素序列，以匹配目标梅尔谱图序列的长度，进而生成并行梅尔谱图。在LJSpeech数据集上的实验表明，我们的并行模型在语音质量方面与自回归模型旗鼓相当，几乎消除了在特别困难的情况下单词跳过和重复的问题，并且可以平滑地调整语音速度。最重要的是，与自回归Transformer TTS相比，我们的模型将梅尔频谱图生成速度提高了270倍，将端到端语音合成速度提高了38倍。因此，我们将我们的模型称为FastSpeech。</p>\n<p><strong>参考</strong><br>[1] <em>Ren Y, Ruan Y, Tan X, et al. Fastspeech: Fast, robust and controllable text to speech[J]. Advances in Neural Information Processing Systems, 2019, 32: 3171-3180.</em><a href=\"https://arxiv.org/abs/1905.09263\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>espnet</em><a href=\"https://github.com/espnet/espnet\">[GitHub]</a><br>[2] <em>Fastspeech_TensorFlow</em><a href=\"https://github.com/TensorSpeech/TensorflowTTS\">[GitHub]</a><br>[3] <em>Fastspeech_PyTorch</em><a href=\"https://github.com/xcmyz/FastSpeech\">[GitHub]</a></p>\n","more":"<p><strong>摘要</strong><br>基于神经网络的端到端文本到语音（TTS）大大提高了合成语音的质量。突出的方法（例如Tacotron2）通常首先从文本生成梅尔频谱图，然后使用诸如WaveNet的声码器从梅尔频谱图合成语音。与传统的拼接和统计参数方法相比，基于神经网络的端到端模型的推理速度较慢，并且合成语音通常不稳定（即某些单词被跳过或重复）且缺乏可控性（语音速度或韵律控制）。在这项工作中，我们提出了一种基于Transformer的新型前馈网络，可为TTS并行生成梅尔频谱图。具体来说，我们从基于编码器-解码器的教师模型中提取注意力对齐，以进行音素持续时间预测，长度调节器将其用于扩展源音素序列，以匹配目标梅尔谱图序列的长度，进而生成并行梅尔谱图。在LJSpeech数据集上的实验表明，我们的并行模型在语音质量方面与自回归模型旗鼓相当，几乎消除了在特别困难的情况下单词跳过和重复的问题，并且可以平滑地调整语音速度。最重要的是，与自回归Transformer TTS相比，我们的模型将梅尔频谱图生成速度提高了270倍，将端到端语音合成速度提高了38倍。因此，我们将我们的模型称为FastSpeech。</p>\n<p><strong>参考</strong><br>[1] <em>Ren Y, Ruan Y, Tan X, et al. Fastspeech: Fast, robust and controllable text to speech[J]. Advances in Neural Information Processing Systems, 2019, 32: 3171-3180.</em><a href=\"https://arxiv.org/abs/1905.09263\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>espnet</em><a href=\"https://github.com/espnet/espnet\">[GitHub]</a><br>[2] <em>Fastspeech_TensorFlow</em><a href=\"https://github.com/TensorSpeech/TensorflowTTS\">[GitHub]</a><br>[3] <em>Fastspeech_PyTorch</em><a href=\"https://github.com/xcmyz/FastSpeech\">[GitHub]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"FastSpeech","path":"api/tags/FastSpeech.json"},{"name":"WaveGlow","path":"api/tags/WaveGlow.json"}]}