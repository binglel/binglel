{"title":"谷歌_CVPR2016_Rethinking the Inception Architecture for Computer Vision","slug":"Paper045","date":"2021-05-10T07:09:42.000Z","updated":"2021-05-10T07:36:12.000Z","comments":true,"path":"api/articles/Paper045.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>卷积网络是用于各种任务的最先进计算机视觉解决方案的核心。自2014年以来，非常深的卷积网络开始成为主流，在各种基准中产生了实质性的收益。尽管增加的模型大小和计算成本往往会为大多数任务带来立竿见影的性能提升（只要为训练提供了足够的标签数据），但计算效率和低参数计数仍然是移动视觉和大数据场景等各种用例的考虑因素。在这里，我们探索通过适当的因子分解卷积和积极的正则化来扩大网络规模的方法，旨在尽可能有效地利用增加的计算。我们在ILSVRC2012分类挑战验证集上对我们的方法进行了基准测试，结果表明，与现有技术相比，我们的方法获得了显著的收益：使用计算成本为50亿次乘加的网络进行单帧评估时，TOP-1和TOP-5的误差分别为21.2%和5.6%，每个推理的计算成本为50亿次乘加，并且使用的参数少于2500万个。在4个模型集成和多种评估的情况下，我们在验证集上报告了3.5%的TOP-5错误(在测试集上错误了3.6%)，在验证集上报告了17.3%的TOP-1错误。</p>\n<p><strong>参考</strong><br>[1] <em>Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2818-2826.</em><a href=\"https://arxiv.org/abs/1512.00567\">[pdf]</a></p>\n","more":"<p><strong>摘要</strong><br>卷积网络是用于各种任务的最先进计算机视觉解决方案的核心。自2014年以来，非常深的卷积网络开始成为主流，在各种基准中产生了实质性的收益。尽管增加的模型大小和计算成本往往会为大多数任务带来立竿见影的性能提升（只要为训练提供了足够的标签数据），但计算效率和低参数计数仍然是移动视觉和大数据场景等各种用例的考虑因素。在这里，我们探索通过适当的因子分解卷积和积极的正则化来扩大网络规模的方法，旨在尽可能有效地利用增加的计算。我们在ILSVRC2012分类挑战验证集上对我们的方法进行了基准测试，结果表明，与现有技术相比，我们的方法获得了显著的收益：使用计算成本为50亿次乘加的网络进行单帧评估时，TOP-1和TOP-5的误差分别为21.2%和5.6%，每个推理的计算成本为50亿次乘加，并且使用的参数少于2500万个。在4个模型集成和多种评估的情况下，我们在验证集上报告了3.5%的TOP-5错误(在测试集上错误了3.6%)，在验证集上报告了17.3%的TOP-1错误。</p>\n<p><strong>参考</strong><br>[1] <em>Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2818-2826.</em><a href=\"https://arxiv.org/abs/1512.00567\">[pdf]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"Inception","path":"api/tags/Inception.json"}]}