{"title":"出门问问_20201210_Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition","slug":"Paper023","date":"2021-01-23T10:26:31.000Z","updated":"2021-03-31T06:19:54.000Z","comments":true,"path":"api/articles/Paper023.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>在本文中，我们提出了一种新型的两遍方法来将流式和非流式端到端（E2E）语音识别统一在一个模型中。该模型采用混合CTC和注意力架构，对编码器中的一致性层进行了修改。我们提出了一种基于动态组块的注意策略来允许输入任意合适的上下文长度。在推理时，CTC解码器以流式方式生成n个最佳假设。只需改变块大小，就可以很容易地控制推理延迟。然后，注意力解码器对CTC假设进行重新评分以得到最终结果。这种高效的重新评分过程只会导致非常小的句子级延迟。在开放170小时的AISHELL-1数据集上的实验表明，该方法能够简单有效地统一流式模型和非流式模型。在AISHELL-1测试集上，与标准非流式Tansformer相比，我们的统一模型在非流式ASR上的相对字符错误率（CER）降低了5.60%。同样的模型在流式ASR系统中获得了5.42%的CER和640ms的延迟。</p>\n<p><strong>参考</strong><br>[1] <em>Zhang B, Wu D, Yao Z, et al. Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition[J]. arXiv preprint arXiv:2012.05481, 2020.</em><a href=\"https://arxiv.org/abs/2012.05481\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>wenet</em><a href=\"https://github.com/mobvoi/wenet\">[GitHub]</a></p>\n","more":"<p><strong>摘要</strong><br>在本文中，我们提出了一种新型的两遍方法来将流式和非流式端到端（E2E）语音识别统一在一个模型中。该模型采用混合CTC和注意力架构，对编码器中的一致性层进行了修改。我们提出了一种基于动态组块的注意策略来允许输入任意合适的上下文长度。在推理时，CTC解码器以流式方式生成n个最佳假设。只需改变块大小，就可以很容易地控制推理延迟。然后，注意力解码器对CTC假设进行重新评分以得到最终结果。这种高效的重新评分过程只会导致非常小的句子级延迟。在开放170小时的AISHELL-1数据集上的实验表明，该方法能够简单有效地统一流式模型和非流式模型。在AISHELL-1测试集上，与标准非流式Tansformer相比，我们的统一模型在非流式ASR上的相对字符错误率（CER）降低了5.60%。同样的模型在流式ASR系统中获得了5.42%的CER和640ms的延迟。</p>\n<p><strong>参考</strong><br>[1] <em>Zhang B, Wu D, Yao Z, et al. Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition[J]. arXiv preprint arXiv:2012.05481, 2020.</em><a href=\"https://arxiv.org/abs/2012.05481\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>wenet</em><a href=\"https://github.com/mobvoi/wenet\">[GitHub]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"End-to-End","path":"api/tags/End-to-End.json"},{"name":"CTC","path":"api/tags/CTC.json"},{"name":"Attention","path":"api/tags/Attention.json"}]}