{"title":"百度_Interspeech2018_Cold Fusion：Training Seq2Seq Models Together with Language Models","slug":"Paper022","date":"2021-01-19T14:37:35.000Z","updated":"2021-01-19T15:03:41.000Z","comments":true,"path":"api/articles/Paper022.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>基于注意力的序列到序列（Seq2Seq）模型在机器翻译、图像字幕和语音识别等生成自然语言句子的任务中表现出色。通过利用未标记的数据（通常以语言模型的形式），进一步提高了性能。在这项工作中，我们提出了冷融合方法，该方法在训练过程中利用预先训练好的语言模型，并在语音识别任务中证明了该方法的有效性。结果表明，采用冷融合的Seq2Seq模型能够更好地利用语言信息，1）具有更快的收敛速度和更好的泛化能力；2）在不到10%的标记训练数据的情况下，几乎完全迁移到新的领域。</p>\n<p><strong>参考</strong><br>[1] <em>Sriram A, Jun H, Satheesh S, et al. Cold fusion: Training seq2seq models together with language models[J]. arXiv preprint arXiv:1708.06426, 2017.</em><a href=\"https://arxiv.org/abs/1708.06426\">[pdf]</a></p>\n","more":"<p><strong>摘要</strong><br>基于注意力的序列到序列（Seq2Seq）模型在机器翻译、图像字幕和语音识别等生成自然语言句子的任务中表现出色。通过利用未标记的数据（通常以语言模型的形式），进一步提高了性能。在这项工作中，我们提出了冷融合方法，该方法在训练过程中利用预先训练好的语言模型，并在语音识别任务中证明了该方法的有效性。结果表明，采用冷融合的Seq2Seq模型能够更好地利用语言信息，1）具有更快的收敛速度和更好的泛化能力；2）在不到10%的标记训练数据的情况下，几乎完全迁移到新的领域。</p>\n<p><strong>参考</strong><br>[1] <em>Sriram A, Jun H, Satheesh S, et al. Cold fusion: Training seq2seq models together with language models[J]. arXiv preprint arXiv:1708.06426, 2017.</em><a href=\"https://arxiv.org/abs/1708.06426\">[pdf]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音识别","path":"api/tags/语音识别.json"},{"name":"Attention","path":"api/tags/Attention.json"},{"name":"Sequence-to-Sequence","path":"api/tags/Sequence-to-Sequence.json"},{"name":"Deep Fusion","path":"api/tags/Deep Fusion.json"},{"name":"Cold Fusion","path":"api/tags/Cold Fusion.json"}]}