{"title":"浙江大学_20200622_FastSpeech 2：Fast and High-Quality End-to-End Text to Speech","slug":"Paper004","date":"2020-12-26T03:45:27.000Z","updated":"2021-03-29T10:08:55.000Z","comments":true,"path":"api/articles/Paper004.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><p><strong>摘要</strong><br>非自回归文本语音转换（TTS）模型（例如FastSpeech）能够比以往的自回归模型以可比的质量明显更快地合成语音。 FastSpeech模型的训练依赖于持续时间预测（以提供更多信息作为输入）和知识蒸馏（以简化输出中的数据分布）的自回归教师模型，这可以缓解一对多映射问题（即多个语音变化对应于TTS中的相同文本）。但是，FastSpeech有几个缺点：（1）师生蒸馏管道复杂且耗时；（2）从老师模型中提取的持续时间不够准确，并且从老师模型中提取的目标梅尔谱图由于数据简化导致信息损失，这两者都限制了语音质量。在本文中，我们提出FastSpeech2，它解决了FastSpeech中的问题，并通过以下方法可以更好地解决TTS中的一对多映射问题：（1）直接训练具有原始频谱的模型，而不是老师模型的简化输出；2）引入更多语音变化信息（例如音调、能量和更准确的持续时间）作为条件输入。具体来说，我们从语音波形中提取持续时间、音调和能量，并将它们直接用作训练中的条件输入，并在推理中使用预测值。我们进一步设计了FastSpeech2s，这是首次尝试直接从文本并行直接生成语音波形，从而享有完全端到端推理的优势。实验结果表明：（1）FastSpeech2的训练速度是FastSpeech的3倍，而FastSpeech2s的推理速度更快。（2）FastSpeech2和2s的语音质量优于FastSpeech，FastSpeech2甚至可以超越自回归模型。</p>\n<p><strong>参考</strong><br>[1] <em>Ren Y, Hu C, Qin T, et al. FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech[J]. arXiv preprint arXiv:2006.04558, 2020.</em><a href=\"https://arxiv.org/abs/2006.04558\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>FastSpeech2</em><a href=\"https://github.com/ming024/FastSpeech2\">[GitHub]</a><br>[2] <em>FastSpeech2</em><a href=\"https://github.com/rishikksh20/FastSpeech2\">[GitHub]</a><br>[3] <em>TensorflowTTS</em><a href=\"https://github.com/TensorSpeech/TensorflowTTS\">[GitHub]</a></p>\n","more":"<p><strong>摘要</strong><br>非自回归文本语音转换（TTS）模型（例如FastSpeech）能够比以往的自回归模型以可比的质量明显更快地合成语音。 FastSpeech模型的训练依赖于持续时间预测（以提供更多信息作为输入）和知识蒸馏（以简化输出中的数据分布）的自回归教师模型，这可以缓解一对多映射问题（即多个语音变化对应于TTS中的相同文本）。但是，FastSpeech有几个缺点：（1）师生蒸馏管道复杂且耗时；（2）从老师模型中提取的持续时间不够准确，并且从老师模型中提取的目标梅尔谱图由于数据简化导致信息损失，这两者都限制了语音质量。在本文中，我们提出FastSpeech2，它解决了FastSpeech中的问题，并通过以下方法可以更好地解决TTS中的一对多映射问题：（1）直接训练具有原始频谱的模型，而不是老师模型的简化输出；2）引入更多语音变化信息（例如音调、能量和更准确的持续时间）作为条件输入。具体来说，我们从语音波形中提取持续时间、音调和能量，并将它们直接用作训练中的条件输入，并在推理中使用预测值。我们进一步设计了FastSpeech2s，这是首次尝试直接从文本并行直接生成语音波形，从而享有完全端到端推理的优势。实验结果表明：（1）FastSpeech2的训练速度是FastSpeech的3倍，而FastSpeech2s的推理速度更快。（2）FastSpeech2和2s的语音质量优于FastSpeech，FastSpeech2甚至可以超越自回归模型。</p>\n<p><strong>参考</strong><br>[1] <em>Ren Y, Hu C, Qin T, et al. FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech[J]. arXiv preprint arXiv:2006.04558, 2020.</em><a href=\"https://arxiv.org/abs/2006.04558\">[pdf]</a></p>\n<p><strong>源码</strong><br>[1] <em>FastSpeech2</em><a href=\"https://github.com/ming024/FastSpeech2\">[GitHub]</a><br>[2] <em>FastSpeech2</em><a href=\"https://github.com/rishikksh20/FastSpeech2\">[GitHub]</a><br>[3] <em>TensorflowTTS</em><a href=\"https://github.com/TensorSpeech/TensorflowTTS\">[GitHub]</a></p>\n","categories":[{"name":"Paper","path":"api/categories/Paper.json"}],"tags":[{"name":"语音合成","path":"api/tags/语音合成.json"},{"name":"合成器","path":"api/tags/合成器.json"},{"name":"声码器","path":"api/tags/声码器.json"},{"name":"WaveGlow","path":"api/tags/WaveGlow.json"},{"name":"FastSpeech2","path":"api/tags/FastSpeech2.json"}]}